{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de21c348",
   "metadata": {},
   "source": [
    "# BEIT visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27490bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "# \n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "# import cv2\n",
    "import random\n",
    "import colorsys\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon, Circle\n",
    "from PIL import Image\n",
    "\n",
    "import skimage.io\n",
    "from skimage.measure import find_contours\n",
    "from scipy import interpolate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "from timm.models import create_model\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "\n",
    "import utils\n",
    "\n",
    "import modeling_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d2f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = image[:, :, c] * (1 - alpha * mask) + alpha * mask * color[c] * 255\n",
    "    return image\n",
    "\n",
    "\n",
    "def random_colors(N, bright=True):\n",
    "    \"\"\"\n",
    "    Generate random colors.\n",
    "    \"\"\"\n",
    "    brightness = 1.0 if bright else 0.7\n",
    "    hsv = [(i / N, 1, brightness) for i in range(N)]\n",
    "    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
    "    random.shuffle(colors)\n",
    "    return colors\n",
    "\n",
    "\n",
    "def display_instances(image, mask, figsize=(5, 5), blur=False, contour=True, alpha=0.5):\n",
    "    fig = plt.figure(figsize=figsize, frameon=False)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    N = 1\n",
    "    mask = mask[None, :, :]\n",
    "    # Generate random colors\n",
    "    colors = random_colors(N)\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    height, width = image.shape[:2]\n",
    "    margin = 0\n",
    "    ax.set_ylim(height + margin, -margin)\n",
    "    ax.set_xlim(-margin, width + margin)\n",
    "    ax.axis('off')\n",
    "    masked_image = image.astype(np.uint32).copy()\n",
    "    for i in range(N):\n",
    "        color = colors[i]\n",
    "        _mask = mask[i]\n",
    "        if blur:\n",
    "            _mask = cv2.blur(_mask,(10,10))\n",
    "        # Mask\n",
    "        masked_image = apply_mask(masked_image, _mask, color, alpha)\n",
    "        # Mask Polygon\n",
    "        # Pad to ensure proper polygons for masks that touch image edges.\n",
    "        if contour:\n",
    "            padded_mask = np.zeros((_mask.shape[0] + 2, _mask.shape[1] + 2))\n",
    "            padded_mask[1:-1, 1:-1] = _mask\n",
    "            contours = find_contours(padded_mask, 0.5)\n",
    "            for verts in contours:\n",
    "                # Subtract the padding and flip (y, x) to (x, y)\n",
    "                verts = np.fliplr(verts) - 1\n",
    "                p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
    "                ax.add_patch(p)\n",
    "    ax.imshow(masked_image.astype(np.uint8), aspect='auto')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f0e3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs, marker=None):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    \n",
    "    # Create subplot grid\n",
    "    n_cols = min(len(imgs), 5)\n",
    "    n_rows = len(imgs) // n_cols + 1\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    fix, axs = plt.subplots(nrows=n_rows, ncols=n_cols, squeeze=True, dpi=150)\n",
    "    missing_imgs = n_rows * n_cols - len(imgs)\n",
    "    for i in range(missing_imgs):\n",
    "        insertion_index = (i+1) * n_cols\n",
    "        imgs = imgs[:insertion_index] + [imgs[0]] + imgs[insertion_index:]\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = F.to_pil_image(img)\n",
    "        axs_index = (i//n_cols, i%n_cols) if n_rows > 1 else i\n",
    "        axs[axs_index].imshow(np.asarray(img))\n",
    "        if marker is not None:\n",
    "            axs[axs_index].add_patch(Circle(marker[::-1], radius=8, color='red'))\n",
    "        axs[axs_index].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "#         axs[i//n_cols, i%n_cols].set_aspect('equal')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.show()\n",
    "# draw_attentions(attentions_normalized[:, :-5], (0,0), THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b00200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_attentions(attentions, threshold, index=0):\n",
    "    \n",
    "    nh = attentions.shape[1] # number of head\n",
    "\n",
    "    # we keep only the output patch attention\n",
    "    attentions = attentions[0, :, index, 1:].reshape(nh, -1)\n",
    "    \n",
    "    th_attn = None\n",
    "    if threshold is not None:\n",
    "        # we keep only a certain percentage of the mass\n",
    "        val, idx = torch.sort(attentions)\n",
    "        val /= torch.sum(val, dim=1, keepdim=True)\n",
    "        cumval = torch.cumsum(val, dim=1)\n",
    "        th_attn = cumval > (1 - threshold)\n",
    "        idx2 = torch.argsort(idx)\n",
    "        for head in range(nh):\n",
    "            th_attn[head] = th_attn[head][idx2[head]]\n",
    "        th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
    "        # interpolate\n",
    "        th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=PATCH_SIZE, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "    attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=PATCH_SIZE, mode=\"nearest\")[0].cpu().numpy()\n",
    "    return attentions, th_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e99a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_beit_model():\n",
    "    model = create_model(\n",
    "        'beit_base_patch16_224',\n",
    "        pretrained=False,\n",
    "        num_classes=21841,\n",
    "        drop_rate=0.0,\n",
    "        drop_path_rate=0.1,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_block_rate=None,\n",
    "        use_mean_pooling=False,\n",
    "        init_scale=0.001,\n",
    "        use_rel_pos_bias=True,\n",
    "        use_shared_rel_pos_bias=False,\n",
    "        use_abs_pos_emb=False,\n",
    "        init_values=0.1,\n",
    "        img_size=INPUT_SIZE\n",
    "    )\n",
    "\n",
    "    patch_size = model.patch_embed.patch_size\n",
    "    print(\"Patch size = %s\" % str(patch_size))\n",
    "    window_size = (INPUT_SIZE // patch_size[0], INPUT_SIZE // patch_size[1])\n",
    "    patch_size = patch_size\n",
    "\n",
    "    if WEIGHT_PATH:\n",
    "        if WEIGHT_PATH.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                WEIGHT_PATH, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(WEIGHT_PATH, map_location='cpu')\n",
    "\n",
    "        print(\"Load ckpt from %s\" % WEIGHT_PATH)\n",
    "        checkpoint_model = None\n",
    "        for model_key in MODEL_KEY.split('|'):\n",
    "            if model_key in checkpoint:\n",
    "                checkpoint_model = checkpoint[model_key]\n",
    "                print(\"Load state_dict by model_key = %s\" % model_key)\n",
    "                break\n",
    "        if checkpoint_model is None:\n",
    "            checkpoint_model = checkpoint\n",
    "        state_dict = model.state_dict()\n",
    "        for k in ['head.weight', 'head.bias']:\n",
    "            if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "        if model.use_rel_pos_bias and \"rel_pos_bias.relative_position_bias_table\" in checkpoint_model:\n",
    "            print(\"Expand the shared relative position embedding to each transformer block. \")\n",
    "            num_layers = model.get_num_layers()\n",
    "            rel_pos_bias = checkpoint_model[\"rel_pos_bias.relative_position_bias_table\"]\n",
    "            for i in range(num_layers):\n",
    "                checkpoint_model[\"blocks.%d.attn.relative_position_bias_table\" % i] = rel_pos_bias.clone()\n",
    "\n",
    "            checkpoint_model.pop(\"rel_pos_bias.relative_position_bias_table\")\n",
    "\n",
    "        all_keys = list(checkpoint_model.keys())\n",
    "        for key in all_keys:\n",
    "            if \"relative_position_index\" in key:\n",
    "                checkpoint_model.pop(key)\n",
    "      \n",
    "            if \"relative_position_bias_table\" in key:\n",
    "                rel_pos_bias = checkpoint_model[key]\n",
    "                src_num_pos, num_attn_heads = rel_pos_bias.size()\n",
    "                dst_num_pos, _ = model.state_dict()[key].size()\n",
    "                dst_patch_shape = model.patch_embed.patch_shape\n",
    "                if dst_patch_shape[0] != dst_patch_shape[1]:\n",
    "                    raise NotImplementedError()\n",
    "                num_extra_tokens = dst_num_pos - (dst_patch_shape[0] * 2 - 1) * (dst_patch_shape[1] * 2 - 1)\n",
    "                src_size = int((src_num_pos - num_extra_tokens) ** 0.5)\n",
    "                dst_size = int((dst_num_pos - num_extra_tokens) ** 0.5)\n",
    "                if src_size != dst_size:\n",
    "                    print(\"Position interpolate for %s from %dx%d to %dx%d\" % (\n",
    "                        key, src_size, src_size, dst_size, dst_size))\n",
    "                    extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n",
    "                    rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n",
    "\n",
    "                    def geometric_progression(a, r, n):\n",
    "                        return a * (1.0 - r ** n) / (1.0 - r)\n",
    "\n",
    "                    left, right = 1.01, 1.5\n",
    "                    while right - left > 1e-6:\n",
    "                        q = (left + right) / 2.0\n",
    "                        gp = geometric_progression(1, q, src_size // 2)\n",
    "                        if gp > dst_size // 2:\n",
    "                            right = q\n",
    "                        else:\n",
    "                            left = q\n",
    "\n",
    "                    # if q > 1.090307:\n",
    "                    #     q = 1.090307\n",
    "\n",
    "                    dis = []\n",
    "                    cur = 1\n",
    "                    for i in range(src_size // 2):\n",
    "                        dis.append(cur)\n",
    "                        cur += q ** (i + 1)\n",
    "\n",
    "                    r_ids = [-_ for _ in reversed(dis)]\n",
    "\n",
    "                    x = r_ids + [0] + dis\n",
    "                    y = r_ids + [0] + dis\n",
    "\n",
    "                    t = dst_size // 2.0\n",
    "                    dx = np.arange(-t, t + 0.1, 1.0)\n",
    "                    dy = np.arange(-t, t + 0.1, 1.0)\n",
    "\n",
    "                    print(\"Original positions = %s\" % str(x))\n",
    "                    print(\"Target positions = %s\" % str(dx))\n",
    "\n",
    "                    all_rel_pos_bias = []\n",
    "\n",
    "                    for i in range(num_attn_heads):\n",
    "                        z = rel_pos_bias[:, i].view(src_size, src_size).float().numpy()\n",
    "                        f = interpolate.interp2d(x, y, z, kind='cubic')\n",
    "                        all_rel_pos_bias.append(\n",
    "                            torch.Tensor(f(dx, dy)).contiguous().view(-1, 1).to(rel_pos_bias.device))\n",
    "\n",
    "                    rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n",
    "\n",
    "                    new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n",
    "                    checkpoint_model[key] = new_rel_pos_bias\n",
    "\n",
    "        # interpolate position embedding\n",
    "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
    "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "        num_patches = model.patch_embed.num_patches\n",
    "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
    "        # height (== width) for the checkpoint position embedding\n",
    "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "        # height (== width) for the new position embedding\n",
    "        new_size = int(num_patches ** 0.5)\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        if orig_size != new_size:\n",
    "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
    "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "            # only the position tokens are interpolated\n",
    "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
    "            pos_tokens = torch.nn.functional.interpolate(\n",
    "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
    "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "            checkpoint_model['pos_embed'] = new_pos_embed\n",
    "\n",
    "        utils.load_state_dict(model, checkpoint_model, prefix='')\n",
    "        # model.load_state_dict(checkpoint_model, strict=False)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c4a72c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch size = (16, 16)\n",
      "Load ckpt from https://unilm.blob.core.windows.net/beit/beit_base_patch16_224_pt22k.pth\n",
      "Load state_dict by model_key = model\n",
      "Expand the shared relative position embedding to each transformer block. \n",
      "Position interpolate for blocks.0.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Position interpolate for blocks.1.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Position interpolate for blocks.2.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Position interpolate for blocks.3.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Position interpolate for blocks.4.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Position interpolate for blocks.5.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Position interpolate for blocks.6.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Position interpolate for blocks.7.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Position interpolate for blocks.8.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Position interpolate for blocks.9.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Position interpolate for blocks.10.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Position interpolate for blocks.11.attn.relative_position_bias_table from 27x27 to 39x39\n",
      "Original positions = [-19.00000568303374, -16.965636986366434, -15.048173102640158, -13.240896107365035, -11.537474120973691, -9.931939124784734, -8.418666051769893, -6.992353078868327, -5.648003051801297, -4.380905977308203, -3.1866225214646713, -2.06096845626831, -1, 0, 1, 2.06096845626831, 3.1866225214646713, 4.380905977308203, 5.648003051801297, 6.992353078868327, 8.418666051769893, 9.931939124784734, 11.537474120973691, 13.240896107365035, 15.048173102640158, 16.965636986366434, 19.00000568303374]\n",
      "Target positions = [-19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.\n",
      "  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.\n",
      "   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.]\n",
      "Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']\n",
      "Weights from pretrained model not used in VisionTransformer: ['mask_token', 'lm_head.weight', 'lm_head.bias']\n",
      "Ignored weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_index', 'blocks.1.attn.relative_position_index', 'blocks.2.attn.relative_position_index', 'blocks.3.attn.relative_position_index', 'blocks.4.attn.relative_position_index', 'blocks.5.attn.relative_position_index', 'blocks.6.attn.relative_position_index', 'blocks.7.attn.relative_position_index', 'blocks.8.attn.relative_position_index', 'blocks.9.attn.relative_position_index', 'blocks.10.attn.relative_position_index', 'blocks.11.attn.relative_position_index']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "PATCH_SIZE = 16\n",
    "MODEL = f'beit_base_patch{PATCH_SIZE}_224'\n",
    "WEIGHT_PATH = f'https://unilm.blob.core.windows.net/beit/{MODEL}_pt22k.pth'\n",
    "INPUT_SIZE = 320\n",
    "MODEL_KEY = 'model|module'\n",
    "\n",
    "model = create_beit_model()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# build model\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6260ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = None\n",
    "ATTENTION_PIXEL = (100,100)  # None\n",
    "\n",
    "def draw_attentions(attentions, attention_pixel, threshold):\n",
    "    if attention_pixel:\n",
    "        feature_pixel = (attention_pixel[0]//PATCH_SIZE, attention_pixel[1]//PATCH_SIZE)\n",
    "        feature_index = (feature_pixel[0] * IMAGE_SIZE[0]//PATCH_SIZE + feature_pixel[1]) + 1\n",
    "    else:\n",
    "        feature_index = 0  # class token\n",
    "    \n",
    "    index_attentions, th_attn = get_index_attentions(attentions, threshold, index=feature_index)\n",
    "\n",
    "    # show attentions heatmaps\n",
    "    plottable_img = torchvision.utils.make_grid(img, normalize=True, scale_each=True)\n",
    "    show([plottable_img] + [attention for attention in index_attentions], marker=attention_pixel)\n",
    "\n",
    "#     if threshold is not None:\n",
    "#     #     image = skimage.io.imread(os.path.join(args.output_dir, \"img.png\"))\n",
    "#         maskable_img = plottable_img.permute(1,2,0).numpy()\n",
    "#         for j in range(nh):\n",
    "#             display_instances(maskable_img, th_attn[j], blur=False)\n",
    "\n",
    "# draw_attentions(ATTENTION_PIXEL, THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "999dc314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a5a6062c0241b995dd79d72fdd13b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Image to use', options=(None, 'README.md', 'beach.webp', 'coco_person.jpeg', 'dashcam.jp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = !ls ../../imagenet-sample-images/\n",
    "images = [None] + images\n",
    "image_selector = widgets.Dropdown(options=images, value=None, description=\"Image to use\")\n",
    "display(image_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9fe85ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--image_path` argument to indicate the path of the image you wish to visualize.\n",
      "Since no image path have been provided, we take the first image in our paper.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_URL = \"https://dl.fbaipublicfiles.com/dino/img.png\"\n",
    "# IMAGE_URL = 'https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n02676566_acoustic_guitar.JPEG'\n",
    "IMAGE_SIZE = (INPUT_SIZE, INPUT_SIZE)\n",
    "\n",
    "image_path = os.path.join('..', '..', 'imagenet-sample-images', image_selector.value) if image_selector.value else None\n",
    "# open image\n",
    "if image_path is None:\n",
    "    # user has not specified any image - we use our own image\n",
    "    print(\"Please use the `--image_path` argument to indicate the path of the image you wish to visualize.\")\n",
    "    print(\"Since no image path have been provided, we take the first image in our paper.\")\n",
    "    response = requests.get(IMAGE_URL)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.convert('RGB')\n",
    "elif os.path.isfile(image_path):\n",
    "    with open(image_path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        img = img.convert('RGB')\n",
    "else:\n",
    "    print(f\"Provided image path {image_path} is non valid.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.Resize(IMAGE_SIZE),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "img = transform(img)\n",
    "\n",
    "# make the image divisible by the patch size\n",
    "w, h = img.shape[1] - img.shape[1] % PATCH_SIZE, img.shape[2] - img.shape[2] % PATCH_SIZE\n",
    "img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "w_featmap = img.shape[-2] // PATCH_SIZE\n",
    "h_featmap = img.shape[-1] // PATCH_SIZE\n",
    "\n",
    "attentions = model.get_last_selfattention(img.to(device))\n",
    "attentions_normalized = attentions / (attentions.mean(axis=-2)[:, :, None, :]+1e-7)\n",
    "# head_sum = attentions_normalized.sum(axis=1)[:, None]\n",
    "# attentions = nn.functional.normalize(attentions, p=1, dim=-1)\n",
    "attentions_normalized = torch.cat((attentions_normalized,\n",
    "                                   attentions_normalized.sum(axis=1)[:, None],\n",
    "                                   attentions.sum(axis=1)[:,None],\n",
    "                                  (attentions > 0.005).sum(axis=1)[:, None]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2a11edc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622c13e51de145748c80f81c6e25f0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='height', max=319), IntSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_slider = widgets.IntSlider(min=0, max=IMAGE_SIZE[0]-1, step=1, description=\"height\", continuous_update=False)\n",
    "w_slider = widgets.IntSlider(min=0, max=IMAGE_SIZE[1]-1, step=1, description=\"width\", continuous_update=False)\n",
    "# th_slider = widgets.FloatSlider(min=0, max=1.0, value=0.0, step=0.05, description=\"threshold\", continuous_update=False)\n",
    "\n",
    "def redraw(h, w, cls):\n",
    "    if cls:\n",
    "        pixel = None\n",
    "    else:\n",
    "        pixel = (h, w)\n",
    "#     display(draw_attentions(attentions_normalized, pixel, THRESHOLD))\n",
    "    display(draw_attentions(attentions_normalized, pixel, THRESHOLD))\n",
    "\n",
    "\n",
    "interactive_plot = interactive(redraw, h=h_slider, w=w_slider, cls=True)\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '500px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd6b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
